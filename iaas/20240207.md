# 2024-02-07

## Participants

- @o-otte
- @horazont
- @boekhorstb1
- @josephineSei
- @lindenb1
- @akafazov
- @cah-hbaum
- @artificial-intelligence
- @garloff
- @brueggemann
- @yeoldegrove
- @fkr
- @ignatov17
- @jschoone
- @michaelbayr
- @berendt
- @sbstnnmnn
- @matusjenca2
- @NotTheEvilOne
- @flyersa
- @frosty-geek
- @bitkeks
- @matfechner
- @b1-luettje
- @frosty-geek

## Housekeeping & Organizational Items

- Report in the community call will be done by: @berendt
- Responsible for the minutes (writing and transfer to github) is: @akafazov


## Agenda

:rotating_light: When adding items to the agenda, please note:

- Please note your handle
- Please note the amount of time the topic needs in order to be discussed adequately
- If your topic requires certain people from our community to be around, please make sure they're present for the meeting

### Cloud Interconnectivity VP04 (@ignatov17)

- 20 Minutes
- @fkr tried to reach out Tellus project - no reponse yet. Alternative approach would be to reach out via C&H.
- goal is to achieve cloud interconnect between tenant networks
- advantages with scaling and resource sharing
- interconnect can leverage BGP EVPN or MPLS VPN provided by the connectivity provider
- traffic routing via SDN router
- look into **BGPVPN Service Plugin** and **OVN BGP Agent**
- open question about use cases:
    - type of connectivity
    - usage scenarios
- current focus only on Openstack based clouds, hyperscalers in the future
- Q @garloff: is the approach too generic? Why focus on hyperscalers?
```
Public cloud aspect came from previous fishing about relevant use cases.
We are focusing on networking layer at this point.
```
- Plusserver/@frosty-geek and Mathias Fechenr: looks good and will investigate further
- StackIT is or was looking into this topic as well -> @horazont to help @fkr to find a contact
- alignment of SCS and Tellus on cloud interconnect could be very helpful (@anjastrunk). @anjastrunk, @frosty-geek, @ignatov17 can align further in March.
- _AI @fkr_: Contact @anjastrunk <-> @ignatov17
- _AI @fkr_: Follow up with Stack-IT once @horazont established contact

[Overview](https://input.scs.community/cloud-interconnectivity-vp04?view) of the current state of research & draft ideas for implementation

### Vote to stabilize IaaS standards (@mbuechse sick, to be substituted for by @cah-hbaum and others)

- 10 minutes
- Preparing the [certification scope](https://docs.scs.community/standards/certification/scopes-versions) [SCS-compatible IaaS](https://docs.scs.community/standards/scs-compatible-iaas) v4 (well ahead of SCS R6)
- to be voted upon in this meeting:
    - stabilize [scs-0101-v1 Entropy](https://docs.scs.community/standards/scs-0101-v1-entropy): [standards/#462](https://github.com/SovereignCloudStack/standards/pull/462) -- @cah-hbaum
        - unanimous
    - stabilize [scs-0103-v1 Standard Flavors and Properties](https://docs.scs.community/standards/scs-0103-v1-standard-flavors): [standards/#463](https://github.com/SovereignCloudStack/standards/pull/463) -- @
        - unanimous
    - stabilize [scs-0104-v1 Standard Images](https://docs.scs.community/standards/scs-0104-v1-standard-images): [standards/#464](https://github.com/SovereignCloudStack/standards/pull/464) -- @berendt
        - Requirement to be able to upload 40+ images with 200GB+ space (per project) is not currently tested in our conformance test: Add this later, open issue
        - deferred
- @garloff: SCS wants to create new certification scope
- Q: standardization is fullfilled when using reference implementation? - Yes
- Metadata definition in the Standard Images
- Q @artificial-intelligence: metadata in custom images? - Only for provider-provided (public) images

### Future of ceph day-2 tooling (@fkr & @boekhorstb1 & @brueggemann & @yeoldegrove)

- 30 Minutes
- Scope: We only discuss the storage solution that comes with the SCS reference implementation here. ADR justifies the choices taken in the reference implementation.
- Rook not suitable due to inability to do zero-downtime in-place migration for existing SCS IaaS deployments (which used OSISM's ceph-ansible)
- B1 Systems involved in SCS has a lot of know-how about ceph, can't we try to work out a solution for the in-place migration just for our SCS deployments?

We want to re-cap the discussion from the overflow on monday. In addition to that we had a few discussions in smaller groups since the overflow and want to bring those points to the table again.

Since it is always hard to timebox those discussions @fkr and @brueggemann will be around longer than the usual meeting slot in order to accomodate for the discussion.

I (@fkr) as well as some colleagues from the community want to really be sure that we decide for a path forward that is not a compromise but that will allow to further disrupt the way day-2 operations are done and how infrastructure is deployed and utilized. From that standpoint I want to see wether rook is a good option.

- Jan-Marten: Support both cephadm (for preexisting SCS OSISM ceph-ansible deployments) and rook (for new deployments)
    - Ideally for a limited time until we figure out how to do cephadm -> rook migration well
    - Or forever (in the worst case)?
- Is rook really the better choice? (@horazont, @mbayr)
    - If you have to master k8s anyway, this is the easier choice
    - If not, cephadm would be the more straight-forward choice
    - Operator skills (@matfechner): Combination of k8s and storage/ceph knowledge may be in short supply?

- @michaelbayr: Rook very good contender compared to ceph-adm
- @garloff: SCS has a goal to defragment open-source technologies. This is done with standardization for the users. For providers, the best approach is to also work on convergence of the software solutions to avoid fragmentation (double work, too thinly spread teams) also there.
- @michaelbayr: Question about reliance on Kubernetes for SCS in the future. 
    > @garloff we already have k3s in any installation and can rely on it.
    - Preference expressed to use it for new services
- rook does work on k3s
- @flyersa expressing preference for long-proven approach taken by OSISM (and go for moving over to k8s slowly step-by-step)
- Testing a migration path ceph-ansible -> rook will be required but may be hard to achieve the same exposure and quality / coverage as for ceph-ansible -> cephadm
    - artcodix willing to invest into testing, development for this
