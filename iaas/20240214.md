# 2024-02-14

## Participants

- @lindenb1
- @ignatov17
- @o-otte
- @flyersa
- @jschoone
- @fdobrovolny
- @josephineSei
- @matfechner
- @gndrmnn
- @yeoldegrove
- @NotTheEvilOne
- @sbstnnmnn
- @garloff (traveling)
- @matusjenca2
- @martinmo
- @b1-luettje
- @horazont
- @mbuechse
- @brueggemann
- @akafazov

## Housekeeping & Organizational Items

- Report in the community call by: @gndrmnn
- Responsible for the minutes (writing and transfer to github): @jschoone


## Agenda

:rotating_light: When adding items to the agenda, please note:

- Please note your handle
- Please note the amount of time the topic needs in order to be discussed adequately
- If your topic requires certain people from our community to be around, please make sure they're present for the meeting

### Encryption between user workloads (@fdobrovolny)

- 10 minutes
- Do we already have experience in doing this?
- We want to create a plugin into Neutron using eBPF to secure the traffic automatically between virtual machines :bee: 
    - there might be cases where traffic is not routed the encrypted way
- We should have a look into upstream to achieve some collaboration on that topic
- SecuStack has experience in encrypting traffic
    - using special SmartNICs which does the encryption, because those are dedicated hardware, not tied to Openstack
    - alternative approach in software (macsec) exists, incomplete
- How does the attacker model looks like?
    - basically the user doesn't have encrypted traffic between workloads
    - someone gaining software access on the compute node the attacker probably has also access to the private key and can decrypt the whole traffic
    - AI @fdobrovolny: create specific attacker models until next meeting
- AI VP04: Check Geneve/VXLAN encryption in OVN
           Encrypting OVN tunnels with IPsec 
           rfc8926 ->  6.1.1. Inter-Data Center Traffic 

### Standard concerning standard images (@mbuechse)

- 10 minutes
- see PR [standards/#472](https://github.com/SovereignCloudStack/standards/pull/472)
    - remove hard-coded least quotas
    - mention fair-use policy instead
- Would this work for you? Could this be stabilized soon?
    - no objections, will proceed to merge this PR, then return to the stabilization proposal

### Future of ceph day-2 tooling (@yeoldegrove)

- 5-10 minutes
- after the ongoing discussions about weather to choose a the k8s-native approach with rook we did some research and groundwork
  - no official solution for a real migration  to rook
    - "rook external cluster" is not on option for us
    - asked for migration path in rook slack-channel
      - no feedback so far
  - we have a POC of a https://github.com/osism/testbed deployment
    - that was migrated from a OSISM deployed `ceph-ansible` to `rook`
      - migrating single MON (export/import monmap)
      - write some rook-yamls, label some k8s-nodes
        - make sure MON is started on migrated node
        - this includes a complete lists of nodes and all (encryped) LVM logical volumes
      - deploy rook
        - single *pre-existing* monitor is started
        - OSDs are just reused and scrubbed
        - scale MONs/MGRs after OSD are in rook
      - data is still there
        - not much evaluation done except that
      - rook is pretty much empty, no pool, no filesystems, no clients, ....
- How are things like CRUSH maps migrated?
    - CRUSH maps are transferred along with mon data
    - Rook only attempts to set the crush tree position of an OSD on first start of that OSD based on k8s topology labels
    - Other than that, Rook doesn't touch the CRUSH map at all, this must be adjusted using ceph cli, e.g. with the rook toolbox
- Custom resources needs to be defined
    - We need to fully test E2E migration that we can retain existing data on ceph, pools have been already tested manually
    - Jonas: Resources can also be defined just on the ceph layer not on the rook layer, rook has some limitations in the capabilities exposed from ceph
        - so some aspects will always be done by ceph-cli
        - rook won't go and delete stuff just because it's missing from the k8s CRDs
    - Kurt: guess we then need to understand what aspects are managed by rook (and properly create the custom resources) and which things are done manually (ceph-cli) anyway
        - migrated clusters may not manage everything through rook that fresh clusters would (the complexity of migration may outweigh the benefits for some aspects)
        - needs to be well documented
