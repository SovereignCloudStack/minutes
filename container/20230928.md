# 2023-09-28
## :family: Participants (github handles preferred)

* @master-caster
* @ppkuschy
* @jschoone
* @berendt
* @matofeder
* @NotTheEvilOne
* @DEiselt
* @fdobrovolny
* @mxmxchere
* @garloff
* @joshmue
* @chess-knight
* @o-otte
* @janiskemper
* @cah-hbaum
* @nitisht
* @batistein

## :telephone: Community call and minutes transfer
* Who is presenting in weekly community call and transforming the pad content to minutes in GitHub?
  * @jschoone

#### [Board of Ljubljana](https://github.com/orgs/SovereignCloudStack/projects/6/views/35?filterQuery=label%3A%22container%22++iteration%3ALjubljana)

## :notebook: Agenda
### How to deal with issues from reference implementation
- last week we said we want to work on bug fixes and migration tasks only, which was clearly too abrupt.
- there are still important issues to work on
- backport issues will be finished
    - of course we do maintenance
- some feature requests should be implemented
    - often clusterapi related so can be reused
- we will go through the whole list to clean it up
    - extract/recycle other important issues which are still valid also for ClusterStacks
    - close no longer important issues

### CAPO 0.8.0 now default :thumbsup:
* in main
    * candidate for maintained/6.x (not 6.0.x)
* clarify: official support for k8s-v1.28?
    * maybe later ... (v1.28 not mentioned, nor OpenStack Zed, Antelope) - ToDo: ask/contribute

### [Renovate Bot](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/577)
- we need a [robot account](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/592)
- associate scs mail address with it as well
- Credential sharing: PGP encrypted text file (on github)? Keepass on nextcloud?
- Self-hosted renovate or just app?
- Proposal:
    - Already have the [app](https://github.com/apps/renovate) in SCS org (but only enabled for generics), has to be enabled on more repositories via https://github.com/organizations/SovereignCloudStack/settings/installations/11140875 in the "Repository access" section (at the moment it's only enabled on generics)
    - If it becomes insufficient, we can switch to self-hosting
        - We don't currently expect that to happen quickly
        - Then we need the extra robot account
    - It makes sense to have a single repository for central configuration, e.g. https://github.com/osism/renovate-config (sample usage for the central repository: https://github.com/osism/terraform-base/blob/main/.github/renovate.json#L2-L5)

### Build and store CAPI images 
- currently done by [OSISM](https://github.com/osism/k8s-capi-images)
    - no plans to stop by OSISM (needed anyway)
- zuul instructions can probably just be copied to us
    - not very difficult (with image-builder) according OSISM
        - using packer?
    - learn how to do this in preparation for taking over at the latest for cluster-stacks
- needs storage

Issue for the HSL thing: https://github.com/kubernetes-sigs/image-builder/issues/1246

### Replace Terraform with OpenTofu
- As a statement, not for technical reasons
- Let's do it! (As soon as first release is available.)

### [Long-term cluster](https://github.com/SovereignCloudStack/issues/issues/435) for the SCS project
#### Why
- As mentioned in the issues to eat our own dogfood
    - this will improve our projects
        - also we can always share the current state of them
- We want to have a community cluster for the project
- Run project specific workloads on e.g.
    - statuspage
    - central api
    - monitoring stack (will be discussed tomorrow in SIG Monitoring btw)
- Auth via UCS? (like Harbor)
    - can also be keycloak if available
        - or should we even host the planned keycloak there? ðŸ¤¯
- We can build a great collaboration across all teams
- Will be a great topic for the [Hackathon](https://events.scs.community/hackathon-3/)! (November 8th at Dresden btw)

#### Where
- Request dedicated project on gxscs (quickest to start with)
- Use a testbed (to have control of all layers -- also have access to keycloak)?
    - Need to provide this with a certain reliability to avoid teams being stalled (by broken infra that noone feels responsible for)
    - SRE will support this

#### How
- We want to get used to Cluster Stacks, but...
    - there's currently only Docker provider supported
    - OpenStack support is planned to follow end of Nov/mid of Dec 
    - will probably result in issues with loadbalancers, storage...
- Idea is to start with the "v1" reference implementation v6.0.0, since this runs better than ever on Openstack
    - Proposal: Transform to cluster classes first so we can more easily move to cluster stacks later (draft PR [k8s-capi/#600](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/pull/600) available)
- get used to Flux since we also provide this, we can use it more
    - enables us to easily install everything on a ClusterStack based cluster later
- get rid of kind by using `clusterctl move`
    - Don't we need a `mgmthost-move.sh` that also moves config + scripts somewhere then?
        - scripts: are under version control i would say
        - config: is still on the local-machine and on the new cluster

### Standards
- K8s version support period issue [issues/#386](https://github.com/SovereignCloudStack/issues/issues/386)
    - PR [standards/#334](https://github.com/SovereignCloudStack/standards/pull/334)
