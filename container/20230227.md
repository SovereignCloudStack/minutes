# 2023-02-27

## Participants (github handles preferred)
* @ppkuschy
* @fkr
* @matfechner
* @batistein
* @chess-knight
* @frosty-geek
* @mxmxchere
* @fynluk
* @maxwolfs
* @jschoone
* @joshmue
* @jnull
* @matofeder
* @saschascherrer
* @mpilka
* @ajfriesen
* @garloff

## Community call report and minutes transfer
* @jschoone

## Agenda

### CAPI Images

* test new CAPI images based on Ubuntu 22.04 (https://github.com/osism/k8s-capi-images)
  * PR: Fix needrestart configuration for management node based on Ubuntu 22.04; [k8s-cluster-api-provider/#326](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/pull/326)
    * Review and merging needed
  * upstream issue, blocking the 22.04 usage for the node images: Ubuntu-2204, qemu - Cloud-init configuration is disabled by the subiquity autoinstaller [kubernetes-sigs/image-builder#1080](https://github.com/kubernetes-sigs/image-builder/issues/1080)
    * Need to wait for upstream fix, likely not in time for R4


### Standardization work

#### Registry
* [standards/#212](https://github.com/SovereignCloudStack/standards/pull/212): ADR Draft by dNation
  * comments have been addressed
  * @joshmue requested for re-review
    * Concern fomr @joshmue is that harbor's multi-tenancy features are less strong (shared storage layer) than some competitors (keppel), which would be relevant in an -as-a-Service offering
    * -as-a-Service *is* relevant for our operators
    * harbor specifically required by some WS customers
      * Shared storage: Security concern? Accounting (billing) concern?
        * Quotas are available (see: https://goharbor.io/docs/main/administration/configure-project-quotas/)
* [k8s-harbor/#23](https://github.com/SovereignCloudStack/k8s-harbor/issues/23): Investigation of Harbor deployment possibilities
  * issue: helm chart vs. operator? -> AI: short ADR on this decision
  * Discuss Tue 11:05
* https://registry.scs.community & SCS Harbor deployment
  * OIDC integration fixed by @garloff (IdP @ scs.sovereignit.de fixed)
  * dNation is able to login, dNation access to the underlying k8s cluster management host will be provided by @garloff 
  * @joshmue, @matofeder, @garloff, @chess-knight will take a meeting tommorow, https://input.scs.community/2023-scs-registry-deployment#


### Reference implementation

#### Bugs
* `kustomize build` broken (`kubectl kustomize` working): PS [k8s/#329](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/pull/329)
  * New version of kustomize (v5) breaks us, kubectl still has an old version, reverting kustomize to an old version (v4) would work as well
  * Need to check whether we need to adapt to be future-proof, lots of deprecation, preferred solution
    * Fallback to pin old version difficult via the l*vely snaps
    * Alternative: Directly install the binary
* We always make sure that the nginx pre-installed with our clusters works, without any additional tweaks users need to be aware of
  * Still should document needed annotations for users that for whatever reason deploy their own nginx ingress

#### Latest versions
* node images, occm+csi: PR [k8s/#327](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/pull/327) - done
* capi+capo: issue [k8s/#330](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/330)
* calico: issue [k8s/#331](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/331)
* cilium: issue [k8s/#332](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/332)
* nginx ingress [k8s/#333](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/333)
* cert manager [k8s/#334](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/334)
* flux [k8s/#335](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/335)
* helm [k8s/#336](https://github.com/SovereignCloudStack/k8s-cluster-api-provider/issues/336)
