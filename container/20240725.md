# 2024-07-25

## :family: Participants (github handles preferred)

- @DEiselt
- @gtema
- @o-otte
- @chess-knight
- @garloff
- @martinmo
- @michal-gubricky
- @janiskemper
- @berendt

## :telephone: Community call and minutes transfer

* Who is reporting in the community call?: @gtema
* Responsible for the minutes (writing and transfer to github): @gtema


## :notebook: Agenda

### Another "user" reports
- deletion of cluster-stacks provisioned cluster stucks on dangling ingress/load_balancer/fip
    - network can not be deleted when ports/fips still exist
    - cluster deletion never completes, leaving user in a 90% deleted state
    - hard to deal with by cluster-stack (because they have not created the LB&FIP)
    - Options:
        - capo could delete everything that belongs to the cluster (even if not created by capo before)
            - difficult ... better error log messages may be the only thing achievable here
            - Related work: https://github.com/giantswarm/cluster-api-cleaner-openstack
                - finalizer
                - unmaintained :-(
        - alternatively refuse to start deletion if anything exists that can't be deleted by capo and error out
            - validation webhook for deletion -> cspo
                - Ideally with error message that hints what needs to be cleaned up
            - Discussion: Is this a generic issue? Resources such as persistent volumes, ingress controllers, ... exist that potentially have created resources in the underlaying infra?
                - Not a performance problem (kubeapi GET is fast, only called on delete)
                - Do we want to avoid that anything gets deleted that the user created?
                    - 1st step: No! Just prevent bad state with leaked resources / hanging deletion when it won't succeed (PVCs, services with type=LB)
                    - Later discussion: `--yes-I-know-what-I-mean-force-it` flag required to delete any user-created things (including containers/pods that are user-created) -> broader discussion needed
    - @gtema to open an issue
- 2 clusters in the same OpenStack project with the same name for ingress controller fight for the single load balancer and steal it from each other
    - ~~capo~~ OCCM should take care
    - avoid name conflict in the first place (prefix cluster-name into it ...)
        - feed different cluster name into OCCM (instead of using default "kubernetes" to construct unique names
        - should be resolved by [CS PR #132](https://github.com/SovereignCloudStack/cluster-stacks/pull/132)


### Cluster Stack Assets can be pushed to OCI registry with csmctl (@janiskemper)
- PRs open
- Allows for sharing assets (main use case: for development, testing, ... avoidind the need to go through github)
- Should work with SCS harbor without any issues

### Cluster Stack Clusters network MTU setting issue
- no possibility to influence/override MTU
    - capo actually allows for it (needs to be verified and exposed) -> @chess-knight
- heavy-handed initcontainer hack looks very heavy
- Issue occured when talking to clusters in other clouds
    - local MTU vs. MTU when talking to the outside


